{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a65d7387",
   "metadata": {},
   "source": [
    "# Transfer Learning on google/vit-base-patch16-224\n",
    "- From HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37051e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --quiet evaluate transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e36cdacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import Dataset\n",
    "from transformers import AutoImageProcessor, ViTForImageClassification\n",
    "from datasets import Image as HFImage\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d0b190",
   "metadata": {},
   "source": [
    "## Load Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc5c09f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_images_path = r\"data/qr_images/qr_images\"\n",
    "qr_labels_path = r\"data/qr_labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71cb3511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_qr_images(filepath: str) -> list:\n",
    "    \"\"\"\n",
    "    Takes in the path to the directory holding all QR images, and return the image file paths as a list\n",
    "    \n",
    "    Args:\n",
    "        filepath: The path to the directory holding the QR images\n",
    "        \n",
    "    Returns:\n",
    "        list: A list holding full paths to QR image files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Defining sorting helper function\n",
    "    def extract_number(filename: str) -> int:\n",
    "        \"\"\"\n",
    "        Extracts the ID or number corresponding to the QR image\n",
    "        \n",
    "        Args:\n",
    "            filename: The QR image\n",
    "            \n",
    "        Returns:\n",
    "            int: The ID or number corresponding to the QR image\n",
    "        \"\"\"\n",
    "        image_number = int(filename.split(\"_\")[1].split(\".\")[0])\n",
    "        return image_number\n",
    "    \n",
    "    # Extracting all items from provided path\n",
    "    all_items = os.listdir(filepath)\n",
    "    \n",
    "    # Keeping only the files from the list\n",
    "    all_files = [f for f in all_items if os.path.isfile(os.path.join(filepath, f))]\n",
    "    \n",
    "    # Numerically sorting the files\n",
    "    sorted_files = sorted(all_files, key=extract_number)\n",
    "    \n",
    "    # Return full paths so downstream code can open files correctly\n",
    "    full_paths = [os.path.join(filepath, f) for f in sorted_files]\n",
    "    return full_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "728f5991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_labels = pd.read_csv(qr_labels_path, index_col=0)\n",
    "labels = y_labels['label'].tolist()\n",
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652ccdce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 1, 0, 0, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hugging Face Lazy List\n",
    "image_paths = get_all_qr_images(qr_images_path)\n",
    "\n",
    "dataset = Dataset.from_dict({\"image\": image_paths, \"label\": labels})\n",
    "dataset = dataset.cast_column(\"image\", HFImage())\n",
    "\n",
    "dataset[255:265]['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2abf1e2",
   "metadata": {},
   "source": [
    "## Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf71171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image processor\n",
    "\"\"\" \n",
    "Prepares input features for vision models and post processing their outputs by \n",
    "    transformations such as resizing, normalization, and conversion to PyTorch \n",
    "    and Numpy tensors.\n",
    "\"\"\"\n",
    "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "processor\n",
    "\n",
    "label2id = ... # Fill in with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a9dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(batch):\n",
    "    \"\"\"\n",
    "    This function prepares a batch of raw image data and labels so they can be fed into a model.\n",
    "    \"\"\"\n",
    "    batch['image'] = [Image.open(io.BytesIO(x['bytes'])).convert('RGB') for x in batch['image']]\n",
    "    inputs = processor(batch['image'],return_tensors='pt')\n",
    "    inputs['labels']=[label2id[y] for y in batch['label']]\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    This function takes a list of inputs dictionaries (from transforms) and stacks them into batch tensors.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616b7587",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b873891",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute evaluation metrics for model predictions using the Hugging Face\n",
    "    `evaluate` library.\n",
    "\"\"\"\n",
    "accuracy = evaluate.load('accuracy')\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits,axis=1)\n",
    "    score = accuracy.compute(predictions=predictions, references=labels)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e57ace",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28eec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\",\n",
    "    num_labels = 2,\n",
    "    id2label = ...,\n",
    "    label2id = ..., \n",
    "    ignore_mismatched_sizes=True\n",
    ")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708bd08b",
   "metadata": {},
   "source": [
    "### Freezing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec06598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze 85,827,109 params\n",
    "for name,p in model.named_parameters():\n",
    "    if not name.startswith('classifier'):\n",
    "        p.requires_grad = False\n",
    "\n",
    "num_params = sum([p.numel() for p in model.parameters()])\n",
    "trainable_params = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "\n",
    "print(f\"{num_params = :,} | {trainable_params = :,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9081a4ea",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de23d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments and train\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vit-base-qr-classifier\",\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=.0001,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=True,\n",
    "    report_to='none',\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=...,\n",
    "    eval_dataset=...,\n",
    "    tokenizer=processor\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1515b89",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663a2a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
