{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a65d7387",
   "metadata": {},
   "source": [
    "# Transfer Learning on google/vit-base-patch16-224\n",
    "- From HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37051e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --quiet evaluate transformers\n",
    "!pip3 install --quiet 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e36cdacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import Dataset\n",
    "from transformers import AutoImageProcessor, ViTForImageClassification\n",
    "from datasets import Image as HFImage\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f90db048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17acb478493b431799646af2ebd9282c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d0b190",
   "metadata": {},
   "source": [
    "## Load Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc5c09f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_images_path = r\"data/qr_images/qr_images\"\n",
    "qr_labels_path = r\"data/qr_labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71cb3511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_qr_images(filepath: str) -> list:\n",
    "    \"\"\"\n",
    "    Takes in the path to the directory holding all QR images, and return the image file paths as a list\n",
    "    \n",
    "    Args:\n",
    "        filepath: The path to the directory holding the QR images\n",
    "        \n",
    "    Returns:\n",
    "        list: A list holding full paths to QR image files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Defining sorting helper function\n",
    "    def extract_number(filename: str) -> int:\n",
    "        \"\"\"\n",
    "        Extracts the ID or number corresponding to the QR image\n",
    "        \n",
    "        Args:\n",
    "            filename: The QR image\n",
    "            \n",
    "        Returns:\n",
    "            int: The ID or number corresponding to the QR image\n",
    "        \"\"\"\n",
    "        image_number = int(filename.split(\"_\")[1].split(\".\")[0])\n",
    "        return image_number\n",
    "    \n",
    "    # Extracting all items from provided path\n",
    "    all_items = os.listdir(filepath)\n",
    "    \n",
    "    # Keeping only the files from the list\n",
    "    all_files = [f for f in all_items if os.path.isfile(os.path.join(filepath, f))]\n",
    "    \n",
    "    # Numerically sorting the files\n",
    "    sorted_files = sorted(all_files, key=extract_number)\n",
    "    \n",
    "    # Return full paths so downstream code can open files correctly\n",
    "    full_paths = [os.path.join(filepath, f) for f in sorted_files]\n",
    "    return full_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "728f5991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_labels = pd.read_csv(qr_labels_path, index_col=0)\n",
    "labels = y_labels['label'].tolist()\n",
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "652ccdce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 1, 0, 0, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hugging Face Lazy List\n",
    "image_paths = get_all_qr_images(qr_images_path)\n",
    "\n",
    "dataset = Dataset.from_dict({\"image\": image_paths, \"label\": labels})\n",
    "dataset = dataset.cast_column(\"image\", HFImage())\n",
    "\n",
    "dataset[255:265]['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16afa9",
   "metadata": {},
   "source": [
    "### Train Val Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5f67f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split = dataset.train_test_split(\n",
    "    test_size=0.2,\n",
    "    #stratify_by_column=\"label\",\n",
    "    seed=42\n",
    "    )\n",
    "\n",
    "train_dataset = dataset_split[\"train\"]\n",
    "test_dataset = dataset_split[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2abf1e2",
   "metadata": {},
   "source": [
    "## Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcf71171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    }
   ],
   "source": [
    "# Load image processor\n",
    "\"\"\" \n",
    "Prepares input features for vision models and post processing their outputs by \n",
    "    transformations such as resizing, normalization, and conversion to PyTorch \n",
    "    and Numpy tensors.\n",
    "\"\"\"\n",
    "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "processor\n",
    "\n",
    "label2id = ... # Fill in with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0a9dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(batch):\n",
    "    \"\"\"\n",
    "    Transforms raw images to pixel_values and passes through labels.\n",
    "    \"\"\"\n",
    "    # 1. Convert images to RGB. \n",
    "    # dataset.cast_column(\"image\", HFImage()) already decoded them to PIL, \n",
    "    # so we just ensure they are RGB.\n",
    "    images = [x.convert(\"RGB\") for x in batch[\"image\"]]\n",
    "\n",
    "    # Process images using the ViT processor\n",
    "    inputs = processor(images, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = batch[\"label\"]\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    This function takes a list of inputs dictionaries (from transforms) and stacks them into batch tensors.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69539199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This applies the preprocessing on-the-fly when data is loaded\n",
    "train_dataset.set_transform(transforms)\n",
    "test_dataset.set_transform(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616b7587",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b873891",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute evaluation metrics for model predictions using the Hugging Face\n",
    "    `evaluate` library.\n",
    "\"\"\"\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits,axis=1)\n",
    "    score = accuracy.compute(predictions=predictions, references=labels)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e57ace",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b28eec67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model initialization\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\",\n",
    "    num_labels = 2,\n",
    "    id2label = {0: \"Phishing\", 1: \"Benign\"},\n",
    "    label2id = {\"Phishing\": 0, \"Benign\": 1}, \n",
    "    ignore_mismatched_sizes=True\n",
    ")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708bd08b",
   "metadata": {},
   "source": [
    "### Freezing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec06598e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_params = 85,800,194 | trainable_params = 1,538\n"
     ]
    }
   ],
   "source": [
    "# Freeze 85,827,109 params\n",
    "for name,p in model.named_parameters():\n",
    "    if not name.startswith('classifier'):\n",
    "        p.requires_grad = False\n",
    "\n",
    "num_params = sum([p.numel() for p in model.parameters()])\n",
    "\n",
    "trainable_params = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "\n",
    "print(f\"{num_params = :,} | {trainable_params = :,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9081a4ea",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de23d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments and train\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vit-base-qr-classifier\",\n",
    "    per_device_train_batch_size=16,\n",
    "    #evaluation_strategy=\"epoch\",\n",
    "    #save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=.0001,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=True,\n",
    "    report_to='none',\n",
    "    #load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=processor\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1515b89",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663a2a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
